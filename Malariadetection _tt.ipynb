{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "abzxBx1by4G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4109aa-bf28-41e9-94fc-7bbc35cd4353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training set: 22046 images (80.00%)\n",
            "Validation set: 2756 images (10.00%)\n",
            "Test set: 2756 images (10.00%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class MalariaDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "         # Convert image to float32 and add channel dimension\n",
        "        image = torch.from_numpy(image).type(torch.float32)\n",
        "        # Add a channel dimension if it's missing\n",
        "        if image.dim() == 2:  # If image has only height and width\n",
        "            image = image.unsqueeze(0)  # Add channel dimension at the beginning\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def process_malaria_images(drive_base_path):\n",
        "    \"\"\"\n",
        "    Processes malaria cell images from a directory.\n",
        "\n",
        "    Args:\n",
        "        drive_base_path (str): Base path where images are stored.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Processed images and their corresponding labels\n",
        "    \"\"\"\n",
        "    parasitized_processed = []\n",
        "    uninfected_processed = []\n",
        "\n",
        "    # Paths to image directories\n",
        "    parasitized_path = os.path.join(drive_base_path, 'Parasitized')\n",
        "    uninfected_path = os.path.join(drive_base_path, 'Uninfected')\n",
        "\n",
        "    target_size = (128, 128)\n",
        "\n",
        "       # Process Parasitized images\n",
        "    for filename in os.listdir(parasitized_path):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(parasitized_path, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            img_resized = cv2.resize(img, target_size)\n",
        "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
        "            _, img_binary = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            parasitized_processed.append(img_binary)\n",
        "\n",
        "    # Process Uninfected images\n",
        "    for filename in os.listdir(uninfected_path):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(uninfected_path, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            img_resized = cv2.resize(img, target_size)\n",
        "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
        "            _, img_binary = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            uninfected_processed.append(img_binary)\n",
        "\n",
        "\n",
        "    # Create labels\n",
        "    X = parasitized_processed + uninfected_processed\n",
        "    y = [1] * len(parasitized_processed) + [0] * len(uninfected_processed)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def split_dataset(X, y, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Split dataset into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        X (list): Input images\n",
        "        y (list): Labels\n",
        "        train_ratio (float): Proportion of training data\n",
        "        val_ratio (float): Proportion of validation data\n",
        "        test_ratio (float): Proportion of test data\n",
        "\n",
        "    Returns:\n",
        "        tuple: Train, validation, and test sets with their corresponding labels\n",
        "    \"\"\"\n",
        "    # First split off test set\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=test_ratio, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Calculate validation proportion relative to train_val set\n",
        "    val_prop = val_ratio / (1 - test_ratio)\n",
        "\n",
        "    # Split remaining data into train and validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=val_prop, random_state=42, stratify=y_train_val\n",
        "    )\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Base path to the folder  cell images\n",
        "drive_base_path = '/content/drive/My Drive/cell_images'\n",
        "\n",
        "# Procesings images and get dataset\n",
        "X, y = process_malaria_images(drive_base_path)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set: {len(X_train)} images ({len(X_train)/len(X)*100:.2f}%)\")\n",
        "print(f\"Validation set: {len(X_val)} images ({len(X_val)/len(X)*100:.2f}%)\")\n",
        "print(f\"Test set: {len(X_test)} images ({len(X_test)/len(X)*100:.2f}%)\")\n",
        "\n",
        "# Create datasets with transforms\n",
        "#transform = transforms.Compose([\n",
        " #   transforms.ToTensor(),\n",
        " #   transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#])\n",
        "\n",
        "train_dataset = MalariaDataset(X_train, y_train)\n",
        "val_dataset = MalariaDataset(X_val, y_val)\n",
        "test_dataset = MalariaDataset(X_test, y_test)\n",
        "\n",
        "# data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing the processed images**"
      ],
      "metadata": {
        "id": "ru2aTXLnnRRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "9SamD9-jwrfv",
        "outputId": "c2b35efd-a69c-40c0-fb3a-be6891a7f10d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "\n",
        "def visualize_one_image_per_class(dataset):\n",
        "    \"\"\"\n",
        "    Visualize one image from each class in the dataset\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # Find indices for each class\n",
        "    parasitized_index = None\n",
        "    uninfected_index = None\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        image, label = dataset[i]\n",
        "        if label == 1 and parasitized_index is None:\n",
        "            parasitized_index = i\n",
        "        elif label == 0 and uninfected_index is None:\n",
        "            uninfected_index = i\n",
        "\n",
        "        # Stop searching if we've found both\n",
        "        if parasitized_index is not None and uninfected_index is not None:\n",
        "            break\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Parasitized image\n",
        "    image_para, label_para = dataset[parasitized_index]\n",
        "    if image_para.dim() == 3:  # If image has channel, height, width\n",
        "        image_para = image_para.squeeze(0).numpy()\n",
        "    elif image_para.dim() == 2:\n",
        "        image_para = image_para.numpy()\n",
        "\n",
        "    # Uninfected image\n",
        "    image_uninf, label_uninf = dataset[uninfected_index]\n",
        "    if image_uninf.dim() == 3:  # If image has channel, height, width\n",
        "        image_uninf = image_uninf.squeeze(0).numpy()\n",
        "    elif image_uninf.dim() == 2:\n",
        "        image_uninf = image_uninf.numpy()\n",
        "\n",
        "    # Plot Parasitized image\n",
        "    ax1.imshow(image_para, cmap='gray')\n",
        "    ax1.set_title('Parasitized (Label: 1)', fontsize=12)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Plot Uninfected image\n",
        "    ax2.imshow(image_uninf, cmap='gray')\n",
        "    ax2.set_title('Uninfected (Label: 0)', fontsize=12)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualizing one binary image from each class in the training dataset\n",
        "visualize_one_image_per_class(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Feature extraction**"
      ],
      "metadata": {
        "id": "ydTIAogqctkx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1cxnSoMwFNM-"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class MalariaClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MalariaClassifier, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(  # Extract features using convolutional layers\n",
        "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Calculate the size of the flattened output from the features\n",
        "        # dummy input with the appropriate shape to get the output size\n",
        "        dummy_input = torch.randn(1, 3, 128, 128)\n",
        "        features_output_size = self.features(dummy_input).shape[1]\n",
        "\n",
        "        self.classifier = nn.Sequential(  # Classify based on extracted features\n",
        "            nn.Linear(features_output_size, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x) #Extract features\n",
        "        x = self.classifier(x) #Classify features\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early Stopping Utility**"
      ],
      "metadata": {
        "id": "3UPnqvhTbtIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6nJzYiKmEvjR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stops the training if validation loss doesn't improve after a given patience\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        \"\"\"\n",
        "        Check if training should stop\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        \"\"\"\n",
        "        Saves model when validation loss decrease.\n",
        "\n",
        "        Args:\n",
        "            val_loss (float): Current validation loss\n",
        "            model (nn.Module): Current model state\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "\n",
        "        # Deep copy the model\n",
        "        self.best_model = copy.deepcopy(model)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Functions**"
      ],
      "metadata": {
        "id": "lweJJ0IEa6i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50):\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize Early Stopping\n",
        "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "    # Track training metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Ensure inputs have the right shape (add channel dimension if needed)\n",
        "            if inputs.dim() == 3:\n",
        "                inputs = inputs.unsqueeze(1)\n",
        "\n",
        "            # Ensure inputs have 3 channels\n",
        "            if inputs.size(1) == 1:\n",
        "                inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Compute average training loss for the epoch\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Ensure inputs have the right shape (add channel dimension if needed)\n",
        "                if inputs.dim() == 3:\n",
        "                    inputs = inputs.unsqueeze(1)\n",
        "\n",
        "                # Ensure inputs have 3 channels\n",
        "                if inputs.size(1) == 1:\n",
        "                    inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                epoch_val_loss += loss.item()\n",
        "\n",
        "        # Compute average validation loss for the epoch\n",
        "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Current Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}\\n')\n",
        "\n",
        "        # Early Stopping\n",
        "        early_stopping(avg_val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    # Return the best model found during training\n",
        "    return train_losses, val_losses, early_stopping.best_model\n",
        "\n",
        "def plot_training_curves(train_losses, val_losses):\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Ensure inputs have the right shape (add channel dimension if needed)\n",
        "            if inputs.dim() == 3:\n",
        "                inputs = inputs.unsqueeze(1)\n",
        "\n",
        "            # Ensure inputs have 3 channels\n",
        "            if inputs.size(1) == 1:\n",
        "                inputs = inputs.repeat(1, 3, 1, 1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100 * correct / total_samples\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}')\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "PDASlwgcbAFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training setup**"
      ],
      "metadata": {
        "id": "U9nboOpwbHpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the model\n",
        "model = MalariaClassifier()\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "# Create learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',   # Look for minimum validation loss\n",
        "    factor=0.75,   # Reduce learning rate by 0.75\n",
        "    patience=1,   # Wait 1 epochs with no improvement\n",
        "    verbose=True, # Print when learning rate changes\n",
        "    min_lr=1e-7   # Minimum learning rate\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TirWWmBkbGYm",
        "outputId": "27b69dca-d727-4f0a-ac31-2b480c831aad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation**"
      ],
      "metadata": {
        "id": "T7eDZINjbUqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "train_losses, val_losses, best_model = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=10  # Increase to allow more room for early stopping\n",
        ")\n",
        "\n",
        "# Plot training curves\n",
        "plot_training_curves(train_losses, val_losses)\n",
        "\n",
        "# Evaluate on test set using the best model\n",
        "test_loss, test_accuracy = evaluate_model(best_model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39lJDxtJbYXk",
        "outputId": "2a36e66b-047c-4566-d5b2-50a7e85a2b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n",
            "Training Loss: 0.5191\n",
            "Validation Loss: 0.4799\n",
            "Current Learning Rate: 0.000010\n",
            "\n",
            "Validation loss decreased (inf --> 0.479936).  Saving model ...\n",
            "Epoch [2/10]\n",
            "Training Loss: 0.4325\n",
            "Validation Loss: 0.4301\n",
            "Current Learning Rate: 0.000010\n",
            "\n",
            "Validation loss decreased (0.479936 --> 0.430150).  Saving model ...\n",
            "Epoch [3/10]\n",
            "Training Loss: 0.3971\n",
            "Validation Loss: 0.4168\n",
            "Current Learning Rate: 0.000010\n",
            "\n",
            "Validation loss decreased (0.430150 --> 0.416764).  Saving model ...\n",
            "Epoch [4/10]\n",
            "Training Loss: 0.3741\n",
            "Validation Loss: 0.4247\n",
            "Current Learning Rate: 0.000010\n",
            "\n",
            "EarlyStopping counter: 1 out of 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PbsZlDyPSqz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}